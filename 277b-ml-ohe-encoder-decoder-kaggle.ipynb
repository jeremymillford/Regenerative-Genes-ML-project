{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10162414,"sourceType":"datasetVersion","datasetId":6275406}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T18:48:17.526345Z","iopub.execute_input":"2024-12-11T18:48:17.526587Z","iopub.status.idle":"2024-12-11T18:48:17.561618Z","shell.execute_reply.started":"2024-12-11T18:48:17.526562Z","shell.execute_reply":"2024-12-11T18:48:17.560818Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gene-sequences-dataframe/gene_sequences.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Necessary Imports","metadata":{}},{"cell_type":"code","source":"# necessary imports\n\n!pip install scikeras\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom keras import layers, models\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout, LSTM, RepeatVector, Reshape, TimeDistributed, Conv1D, MaxPooling1D, Flatten\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nfrom scipy.stats import ttest_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:02:29.170263Z","iopub.execute_input":"2024-12-11T17:02:29.171234Z","iopub.status.idle":"2024-12-11T17:02:41.142526Z","shell.execute_reply.started":"2024-12-11T17:02:29.171180Z","shell.execute_reply":"2024-12-11T17:02:41.141499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function definitions \n\ndef onehote(sequence):\n    # map each nucelotide in the string as a number \n    mapping = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3, \"U\": 3}\n\n    one_hot_encoded = []\n\n    for nuc in sequence:\n        # Check if nucleotide is valid \n        if nuc in mapping:\n\n            encoded = np.eye(4)[mapping[nuc]]  \n            one_hot_encoded.append(encoded) \n        else:\n            # If nucleotide is not valid, skip it\n            print(f\"Warning: Invalid nucleotide '{nuc}' encountered. Skipping.\")\n            return None\n        \n    return np.array(one_hot_encoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:02:41.144283Z","iopub.execute_input":"2024-12-11T17:02:41.145267Z","iopub.status.idle":"2024-12-11T17:02:41.150835Z","shell.execute_reply.started":"2024-12-11T17:02:41.145219Z","shell.execute_reply":"2024-12-11T17:02:41.149875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for TPU availability\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)  # Create TPU strategy\n    print(\"Running on TPU:\", tpu.master())\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()  # Default strategy if TPU is not available\n    print(\"Running on default strategy\")\n\nprint(f\"Number of replicas: {strategy.num_replicas_in_sync}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure GPU\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        tf.config.set_visible_devices(gpus[0], 'GPU')\n        print(\"Using GPU:\", gpus[0])\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"No GPU found!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:02:51.959517Z","iopub.execute_input":"2024-12-11T17:02:51.960406Z","iopub.status.idle":"2024-12-11T17:02:52.037641Z","shell.execute_reply.started":"2024-12-11T17:02:51.960368Z","shell.execute_reply":"2024-12-11T17:02:52.036541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load Data and Change Structure","metadata":{}},{"cell_type":"code","source":"features_df = pd.read_csv(\"/kaggle/input/gene-sequences-dataframe/gene_sequences.csv\")\nfeatures_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:03:01.568758Z","iopub.execute_input":"2024-12-11T17:03:01.569165Z","iopub.status.idle":"2024-12-11T17:03:01.789292Z","shell.execute_reply.started":"2024-12-11T17:03:01.569133Z","shell.execute_reply":"2024-12-11T17:03:01.788282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop gene_name and separate features and target \nX = features_df.drop(columns=['gene_name'])\ny = (features_df['regen']).astype(int)  \ny = y.to_numpy()\n\n# Choose only numeric features for modeling\nX_col = X.columns\nprint(X_col)\nX_numeric_features = [\"sequence_length\", \"gc_content\", \"at_gc_ratio\", 'kmer_3_GGG',\n       'kmer_3_GGC', 'kmer_3_GCG', 'kmer_3_CGG', 'kmer_3_GCT', 'kmer_3_CTG',\n       'kmer_3_TGC', 'kmer_3_GCC', 'kmer_3_CCG', 'kmer_3_CGC', 'kmer_3_CCA',\n       'kmer_3_CAG', 'kmer_3_AGC', 'kmer_3_CCC', 'kmer_3_CTA', 'kmer_3_TAA',\n       'kmer_3_AAG', 'kmer_3_AGG', 'kmer_3_CTC', 'kmer_3_TCG', 'kmer_3_GGA',\n       'kmer_3_GAG', 'kmer_3_AGA', 'kmer_3_TCT', 'kmer_3_TGT', 'kmer_3_GTT',\n       'kmer_3_TTT', 'kmer_3_TTC', 'kmer_3_TCC', 'kmer_3_AGT', 'kmer_3_GTC',\n       'kmer_3_GTG', 'kmer_3_TGG', 'kmer_3_CTT', 'kmer_3_TTA', 'kmer_3_AAA',\n       'kmer_3_CAC', 'kmer_3_ACT', 'kmer_3_CGT', 'kmer_3_CAA', 'kmer_3_AAT',\n       'kmer_3_ATG', 'kmer_3_TAT', 'kmer_3_TGA', 'kmer_3_GAC', 'kmer_3_ACA',\n       'kmer_3_TCA', 'kmer_3_CCT', 'kmer_3_ACG', 'kmer_3_GAT', 'kmer_3_ATC',\n       'kmer_3_CGA', 'kmer_3_CAT', 'kmer_3_TAC', 'kmer_3_AAC', 'kmer_3_GCA',\n       'kmer_3_GAA', 'kmer_3_ACC', 'kmer_3_GGT', 'kmer_3_ATT', 'kmer_3_TTG',\n       'kmer_3_ATA', 'kmer_3_GTA', 'kmer_3_TAG', 'kmer_3_CAN', 'kmer_3_ANN',\n       'kmer_3_NNN', 'kmer_3_NNT', 'kmer_3_NTA']\n\nX = X.drop(columns = X_numeric_features)\nX.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:03:02.080734Z","iopub.execute_input":"2024-12-11T17:03:02.081655Z","iopub.status.idle":"2024-12-11T17:03:02.094479Z","shell.execute_reply.started":"2024-12-11T17:03:02.081617Z","shell.execute_reply":"2024-12-11T17:03:02.093485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# determine longest sequence\nmax_len = 0\nfor seq in X['sequence']:\n    seq_len = len(seq)\n    if seq_len > max_len:\n        max_len = seq_len ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# one hot encode each sequence and store in a NumPy array\nsequences_ohe = []\nupdated_Y = [] \nfor i, seq in enumerate(X['sequence']):\n    output = onehote(seq)\n    if output is None:\n        continue\n    else:\n        sequences_ohe.append(output)\n        updated_Y.append(y[i]) \nsequences_X_padded = pad_sequences(sequences_ohe, padding='post', maxlen = max_len, dtype='float32', value=0)\n\n# Replace the original Y_train with the updated one\nY = np.array(updated_Y)\n\n# train, test split\nX_train, X_test, Y_train, Y_test = train_test_split(sequences_X_padded, Y, test_size=0.2, random_state=46)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:03:06.156783Z","iopub.execute_input":"2024-12-11T17:03:06.157166Z","iopub.status.idle":"2024-12-11T17:03:43.499361Z","shell.execute_reply.started":"2024-12-11T17:03:06.157133Z","shell.execute_reply":"2024-12-11T17:03:43.498609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert to tensors for TPU\n\nX_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\nX_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\nY_train = tf.convert_to_tensor(Y_train, dtype=tf.float32)\nY_test = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n\n# adjust the dataset to require lower memory\nbatch_size = 4\n\n# Use tf.data for better input pipeline performance\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n\nprint(X_train.shape)  # Check shape of input\nprint(Y_train.shape)  # Check shape of labels\nprint(X_test.shape)  # Check shape of input\nprint(Y_test.shape)  # Check shape of labels\n\ninput_shape = X_train.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:03:57.112016Z","iopub.execute_input":"2024-12-11T17:03:57.112692Z","iopub.status.idle":"2024-12-11T17:04:08.291741Z","shell.execute_reply.started":"2024-12-11T17:03:57.112658Z","shell.execute_reply":"2024-12-11T17:04:08.290859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model 1: Simple CNN/LSTM","metadata":{}},{"cell_type":"code","source":"# Encoder\ninput_layer = Input(shape=(input_shape[1], input_shape[2]))\nx = Conv1D(filters=16, kernel_size=3, activation='relu')(input_layer)\nx = MaxPooling1D(pool_size=4)(x)\nx = Dropout(0.3)(x)\n\n# LSTM for Encoder\nencoded = LSTM(32, activation='tanh', recurrent_activation='sigmoid')(x)\n\n# Decoder\ndecoded = RepeatVector(input_shape[1])(encoded)\ndecoded = LSTM(32, return_sequences=True, activation='tanh', recurrent_activation='sigmoid')(decoded)\ndecoded = Dropout(0.3)(decoded)\n\n# Output Layer\noutput_layer = TimeDistributed(Dense(1, activation='sigmoid'))(decoded)\n\n# Model\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\n\n# Define EarlyStopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_loss',     # Metric to monitor (validation loss in this case)\n    patience=3,             # Number of epochs with no improvement after which training stops\n    restore_best_weights=True  # Restores model weights from the epoch with the best validation loss\n)\n\n# Compile the Model\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train with GPU\nwith tf.device('/GPU:0'):\n    autoencoder.fit(\n        train_dataset,\n        epochs=10,\n        validation_data=validation_dataset,\n        callbacks=[early_stopping],\n        verbose=1\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T17:06:57.756487Z","iopub.execute_input":"2024-12-11T17:06:57.756891Z","iopub.status.idle":"2024-12-11T17:30:35.888066Z","shell.execute_reply.started":"2024-12-11T17:06:57.756858Z","shell.execute_reply":"2024-12-11T17:30:35.885566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize results\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title(\"Training History: CNN/LSTM with Encoder and Decoder Network\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\ny_pred = model.predict(X_test)\ny_pred_binary = (y_pred >= 0.5).astype(int)\n\ncm = confusion_matrix(Y_test, y_pred_binary)\n# Plot confusion matrix\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Not Associated\", \"Associated\"], yticklabels=[\"Not Associated\", \"Associated\"])\nplt.title(\"Confusion Matrix: All Numeric Features with GridSearch\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# Classification report\nprint(classification_report(Y_test, y_pred_binary, target_names=[\"Not Associated\", \"Associated\"]))\n\nprint(\"Test Accuracy:\", accuracy_score(Y_test, y_pred_binary))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}